{
 "cells": [
  {
   "cell_type": "raw",
   "id": "526fee8c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%pip install -q langchain_community faiss-cpu unsloth torch_xla transformers==4.55.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31a8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def sanitizeText(stringText):\n",
    "  if isinstance(stringText, str): return re.sub(r'[^A-Za-z0-9\\s]+', '', stringText).strip()\n",
    "  else: return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a50113c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>reviews</th>\n",
       "      <th>bookContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toxic Turkey</td>\n",
       "      <td>Toxic Turkey by Gage Irving is a captivating m...</td>\n",
       "      <td>Toxic Turkey\\n“Infinite wealth waited for them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love All the Way</td>\n",
       "      <td>Love All the Way is labeled as a romance novel...</td>\n",
       "      <td>Love All the Way\\nCopyright © 2021 by Aurora C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Love All the Way</td>\n",
       "      <td>Melanie Foster and Allen Shandi met on Septemb...</td>\n",
       "      <td>Love All the Way\\nCopyright © 2021 by Aurora C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Love All the Way</td>\n",
       "      <td>Melanie, a fiery young woman, meets Allen when...</td>\n",
       "      <td>Love All the Way\\nCopyright © 2021 by Aurora C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love All the Way</td>\n",
       "      <td>Love All the Way by Aurora Carafe is a heartwa...</td>\n",
       "      <td>Love All the Way\\nCopyright © 2021 by Aurora C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bookTitle                                            reviews  \\\n",
       "0      Toxic Turkey  Toxic Turkey by Gage Irving is a captivating m...   \n",
       "1  Love All the Way  Love All the Way is labeled as a romance novel...   \n",
       "2  Love All the Way  Melanie Foster and Allen Shandi met on Septemb...   \n",
       "3  Love All the Way  Melanie, a fiery young woman, meets Allen when...   \n",
       "4  Love All the Way  Love All the Way by Aurora Carafe is a heartwa...   \n",
       "\n",
       "                                         bookContent  \n",
       "0  Toxic Turkey\\n“Infinite wealth waited for them...  \n",
       "1  Love All the Way\\nCopyright © 2021 by Aurora C...  \n",
       "2  Love All the Way\\nCopyright © 2021 by Aurora C...  \n",
       "3  Love All the Way\\nCopyright © 2021 by Aurora C...  \n",
       "4  Love All the Way\\nCopyright © 2021 by Aurora C...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from json import load\n",
    "with open('bookSummaries.json','r',encoding='utf-16') as file: bookSummaries = load(file)\n",
    "with open('bookReviews.json','r',encoding='utf-16') as file: reviewTexts2 = load(file)\n",
    "import pandas as pd\n",
    "bookContent = pd.DataFrame(bookSummaries,columns=['bookTitle','bookContent'])\n",
    "bookContent['bookTitle'] = bookContent['bookTitle'].apply(sanitizeText)\n",
    "bookContent = bookContent[bookContent['bookContent'].apply(lambda x: len(str(x).split())>10)].reset_index(drop=True)\n",
    "bookContent.drop_duplicates().reset_index(drop=True,inplace=True)\n",
    "bookContent['bookContent'] = bookContent['bookContent'].apply(lambda x: x.replace('Back to store\\n',''))\n",
    "bookReviews = pd.DataFrame(reviewTexts2,columns=['bookTitle','reviews'])\n",
    "bookReviews['bookTitle'] = bookReviews['bookTitle'].apply(sanitizeText)\n",
    "merged_books_df = pd.merge(bookReviews, bookContent, on='bookTitle', how='inner')\n",
    "display(merged_books_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdee0646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EleutherAI/gpt-neo-125M\n",
      "Model max length: 2048\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_max_length = model.config.max_position_embeddings\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Model max length: {model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf46e793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: sshleifer/distilbart-cnn-12-6\n",
      "Model max length: 1024\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "modelName = \"sshleifer/distilbart-cnn-12-6\"  # Smaller BART model for summarization\n",
    "summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(modelName)\n",
    "summarizer_tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "\n",
    "# Model's max length\n",
    "summarizer_model_max_length = summarizer_model.config.max_position_embeddings\n",
    "print(f\"Model: {modelName}\")\n",
    "print(f\"Model max length: {summarizer_model_max_length}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "138ef3a9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import nltk\n",
    "import torch\n",
    "\n",
    "def summarize_review_manual(row):\n",
    "    book_title = row['bookTitle']\n",
    "    review_text = row['reviews']\n",
    "    max_review_length = model_max_length - len(f\"Generate a review for the book: {book_title}\")\n",
    "    summarizer_max_input_length = summarizer_model_max_length\n",
    "\n",
    "    # Determine device - explicitly set to CPU\n",
    "    device = torch.device(\"cpu\") # Changed device to CPU\n",
    "\n",
    "    if len(review_text) > max_review_length:\n",
    "        # Split the review into sentences\n",
    "        sentences = nltk.sent_tokenize(review_text)\n",
    "        summarized_chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Check if adding the current sentence exceeds the summarizer's max input length\n",
    "            # Add a small buffer to avoid exceeding max input length exactly\n",
    "            if len(summarizer_tokenizer.encode(current_chunk + sentence)) < summarizer_max_input_length - 5:\n",
    "                current_chunk += (sentence + \" \")\n",
    "            else:\n",
    "                # Summarize the current chunk\n",
    "                print(f\"Summarizing chunk of length (tokens): {len(summarizer_tokenizer.encode(current_chunk))}\")\n",
    "                # Manually tokenize and move to device\n",
    "                inputs = summarizer_tokenizer(current_chunk, return_tensors=\"pt\", max_length=summarizer_max_input_length, truncation=False).to(device)\n",
    "\n",
    "                # Generate summary\n",
    "                summary_ids = summarizer_model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    max_length=min(summarizer_max_input_length, inputs[\"input_ids\"].shape[1] + 50), # Generate summary up to input length + buffer\n",
    "                    min_length=30,\n",
    "                    num_beams=4, # Use beam search for potentially better summaries\n",
    "                    early_stopping=True\n",
    "                )\n",
    "                summarized_chunk = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "                summarized_chunks.append(summarized_chunk)\n",
    "\n",
    "                # Start a new chunk with the current sentence\n",
    "                current_chunk = (sentence + \" \")\n",
    "\n",
    "        # Summarize the last chunk if it's not empty\n",
    "        if current_chunk:\n",
    "            print(f\"Summarizing final chunk of length (tokens): {len(summarizer_tokenizer.encode(current_chunk))}\")\n",
    "            # Manually tokenize and move to device\n",
    "            inputs = summarizer_tokenizer(current_chunk, return_tensors=\"pt\", max_length=summarizer_max_input_length, truncation=False).to(device)\n",
    "            # Generate summary\n",
    "            summary_ids = summarizer_model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=min(summarizer_max_input_length, inputs[\"input_ids\"].shape[1] + 50), # Generate summary up to input length + buffer\n",
    "                min_length=30,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            summarized_chunk = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            summarized_chunks.append(summarized_chunk)\n",
    "\n",
    "\n",
    "        # Concatenate the summarized chunks\n",
    "        summarized_text = \" \".join(summarized_chunks)\n",
    "\n",
    "        # Keep summarizing the combined chunks until the output meets the length criterion for the main model\n",
    "        # Summarize the combined text\n",
    "        while len(summarizer_tokenizer.encode(summarized_text)) > max_review_length:\n",
    "             print(f\"Summarizing combined text of length (tokens): {len(summarizer_tokenizer.encode(summarized_text))}\")\n",
    "             # Manually tokenize and move to device\n",
    "             inputs = summarizer_tokenizer(summarized_text, return_tensors=\"pt\", max_length=summarizer_max_input_length, truncation=False).to(device)\n",
    "             # Generate summary\n",
    "             summary_ids = summarizer_model.generate(\n",
    "                 inputs[\"input_ids\"],\n",
    "                 max_length=min(summarizer_max_input_length, inputs[\"input_ids\"].shape[1] + 50), # Generate summary up to input length + buffer\n",
    "                 min_length=30,\n",
    "                 num_beams=4,\n",
    "                 early_stopping=True\n",
    "             )\n",
    "             summarized_text = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "        return summarized_text\n",
    "    else:\n",
    "        return review_text\n",
    "\n",
    "# Apply the function to the 'reviews' column\n",
    "# Iterate through rows and apply the function\n",
    "processed_reviews_manual = []\n",
    "for index, row in merged_books_df.drop_duplicates(subset=['reviews']).iterrows(): \n",
    "  processed_reviews_manual.append((row['bookTitle'], \n",
    "row['bookContent'],\n",
    "summarize_review_manual(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1032b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df = pd.DataFrame(processed_reviews_manual,columns=['bookTitle','bookContent','reviews'])#.to_csv('bookReview_summarized.csv',index=False)\n",
    "merged_df = pd.read_csv('bookReview_summarized.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bf9624a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def combine_text(row):\n",
    "    return f\"Book Title: {row['bookTitle']}\\nBook Content: {row['bookContent']}\\nReview: {row['reviews']}\"\n",
    "\n",
    "merged_df['combined_text'] = merged_df.apply(combine_text, axis=1)\n",
    "\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(merged_df)\n",
    "\n",
    "# Set a padding token for the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the combined text\n",
    "def tokenize_function(examples):\n",
    "    # Use the tokenizer loaded earlier\n",
    "    # Disable truncation and set padding\n",
    "    return tokenizer(examples[\"combined_text\"], truncation=False, padding=\"max_length\", max_length=model_max_length)\n",
    "\n",
    "# Remove '__index_level_0__' from the remove_columns list (if it exists after using merged_df)\n",
    "# Based on previous error, it's safer to assume it won't be there\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['bookTitle', 'reviews', 'bookContent', 'combined_text'])\n",
    "\n",
    "# Print some information about the tokenized dataset\n",
    "print(tokenized_dataset)\n",
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5c86e59",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Split the tokenized dataset into training and validation sets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd55c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/349 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 349/349 [00:00<00:00, 446.27 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 349/349 [00:00<00:00, 1068.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 90\n",
      "})\n",
      "{'input_ids': [10482, 11851, 25, 29274, 7137, 198, 10482, 14041, 25, 29274, 7137, 198, 447, 250, 18943, 9504, 5129, 13488, 329, 606, 379, 262, 5461, 1627, 198, 259, 257, 3234, 326, 40424, 606, 656, 4692, 12, 50132, 5123, 13, 447, 251, 198, 38, 496, 26101, 198, 51, 18047, 7137, 198, 15269, 10673, 3738, 36743, 5857, 11739, 198, 1797, 15766, 25, 41417, 12, 16, 12, 5607, 486, 4310, 12, 2920, 12, 16, 198, 20344, 3890, 25, 44211, 4897, 5834, 198, 27245, 7412, 25, 37356, 198, 2949, 636, 286, 428, 9207, 743, 307, 31759, 393, 18307, 287, 597, 1296, 393, 416, 597, 1724, 11, 13028, 11, 7914, 11, 49857, 11081, 11, 8296, 11, 393, 416, 597, 1321, 6143, 45069, 1080, 960, 16341, 329, 39146, 973, 329, 3199, 2423, 960, 19419, 262, 3194, 7170, 286, 262, 6434, 13, 198, 14772, 6669, 1653, 23499, 198, 53, 3529, 8511, 11, 4744, 198, 464, 38315, 2304, 385, 2254, 198, 75, 1689, 1653, 12984, 20020, 31, 14816, 13, 785, 198, 35, 276, 3474, 284, 6219, 38005, 198, 2964, 75, 5119, 198, 1157, 25, 2327, 3001, 3431, 11, 1737, 1511, 11, 12113, 198, 49, 15799, 32666, 11, 16033, 198, 47, 7958, 262, 2524, 1022, 734, 1402, 18180, 11, 484, 3706, 23915, 290, 38681, 17704, 26, 8759, 263, 32666, 373, 5201, 287, 1596, 5999, 13, 1119, 3170, 1115, 1036, 396, 76, 2171, 1262, 262, 1176, 286, 262, 18180, 284, 20078, 262, 13020, 13, 11597, 3111, 880, 1576, 1566, 257, 6947, 6572, 749, 286, 511, 5682, 290, 477, 262, 41999, 287, 1248, 1983, 13, 1320, 1241, 286, 8166, 1908, 606, 1497, 11, 290, 340, 373, 922, 484, 447, 247, 67, 795, 38769, 503, 13, 317, 1218, 6388, 7560, 1194, 24726, 6947, 5193, 812, 1568, 11, 20518, 2048, 2279, 2073, 1497, 13, 1318, 2492, 447, 247, 83, 881, 286, 262, 3240, 1364, 287, 262, 4295, 16479, 286, 16033, 25, 257, 10905, 3240, 1327, 284, 1064, 13, 198, 464, 5638, 10711, 40425, 10840, 373, 287, 7703, 5866, 1812, 3250, 13, 8759, 263, 32666, 373, 1440, 4608, 1497, 422, 262, 22115, 13, 1649, 262, 1181, 19233, 262, 1956, 1088, 262, 22115, 11, 340, 6492, 262, 3240, 832, 37820, 7386, 13, 16847, 44001, 749, 286, 262, 32666, 287, 9507, 11, 351, 691, 530, 6631, 13, 29602, 287, 5638, 10711, 12086, 8759, 263, 32666, 11, 290, 484, 8853, 276, 262, 1957, 1230, 284, 1309, 25017, 4599, 439, 447, 247, 82, 2156, 1302, 13, 383, 6156, 26765, 3332, 319, 262, 4511, 22910, 1022, 262, 18180, 11, 6193, 278, 832, 262, 27283, 13, 5334, 6961, 373, 6292, 11, 5051, 4035, 262, 2156, 355, 257, 6754, 17757, 13, 1400, 530, 561, 766, 4599, 439, 447, 247, 82, 2156, 706, 262, 30164, 3259, 1364, 262, 1989, 11, 523, 340, 1107, 373, 257, 27158, 28251, 13, 1629, 1551, 484, 1392, 262, 1334, 286, 262, 670, 1760, 5443, 290, 11721, 13, 198, 40630, 40908, 276, 656, 4647, 13, 1400, 530, 2982, 262, 1468, 2156, 1126, 461, 290, 19680, 13, 1400, 530, 1816, 503, 612, 13, 1400, 530, 12086, 262, 1295, 7471, 13, 383, 28902, 422, 262, 1036, 396, 76, 2171, 550, 9292, 284, 262, 4220, 286, 262, 18180, 11, 655, 23273, 11945, 355, 262, 2877, 9353, 286, 262, 8222, 356, 9586, 656, 373, 1364, 286, 8759, 263, 32666, 284, 14083, 290, 23875, 13, 383, 4599, 439, 1363, 28178, 625, 49333, 1399, 28446, 21083, 28077, 257, 10574, 10436, 11, 33971, 257, 2700, 326, 550, 587, 46681, 77, 803, 612, 329, 4647, 13, 198, 38454, 8369, 7521, 319, 262, 7714, 286, 465, 4928, 16968, 683, 510, 13, 15890, 1718, 465, 10147, 572, 11, 37432, 340, 319, 257, 5445, 13990, 1281, 11, 3772, 284, 3443, 670, 287, 257, 309, 12, 15600, 13, 632, 447, 247, 82, 587, 1165, 4692, 284, 466, 326, 329, 1933, 13, 2293, 1115, 812, 286, 670, 11, 465, 3240, 373, 2048, 1760, 960, 4053, 11, 262, 24685, 16175, 671, 286, 340, 6949, 13, 679, 447, 247, 67, 3170, 1936, 7777, 11, 257, 2276, 3650, 11, 290, 257, 4928, 319, 281, 8593, 9586, 2610, 339, 1444, 564, 246, 12417, 4675, 13, 447, 247, 198, 1544, 1234, 262, 7521, 2485, 866, 290, 1761, 9124, 625, 284, 262, 3996, 286, 465, 7779, 284, 1064, 465, 17626, 13, 1119, 547, 29445, 2004, 739, 465, 2891, 3524, 13, 679, 6578, 530, 290, 23831, 1028, 262, 1735, 286, 465, 376, 14877, 11, 33679, 278, 503, 7523, 13917, 13, 1629, 16571, 12, 11545, 11, 465, 1767, 550, 7082, 46258, 306, 284, 465, 22286, 11, 290, 465, 4190, 290, 467, 378, 68, 691, 550, 257, 1178, 48356, 286, 13791, 284, 12127, 883, 812, 13, 198, 43, 3543, 287, 257, 2717, 3770, 287, 6035, 10711, 11, 14493, 11, 422, 16101, 284, 15524, 11, 15890, 1004, 14029, 550, 587, 900, 1479, 826, 706, 465, 294, 2265, 19235, 10955, 13, 679, 447, 247, 67, 4504, 284, 5103, 11, 7463, 8208, 12, 26022, 3625, 572, 257, 18002, 319, 257, 1693, 287, 510, 5219, 968, 1971, 287, 14745, 11, 7163, 465, 10359, 290, 14511, 13, 679, 373, 287, 257, 33658, 329, 625, 257, 1227, 13, 383, 5096, 1664, 373, 29793, 276, 618, 339, 19092, 510, 13, 383, 22536, 12, 354, 2313, 9326, 2952, 1043, 1223, 2642, 351, 262, 18002, 13, 383, 1664, 290, 262, 8517, 1111, 10282, 503, 286, 2184, 11, 5989, 683, 5193, 1510, 5054, 287, 1128, 24355, 257, 614, 706, 262, 5778, 13, 15890, 4444, 510, 351, 1440, 1510, 5054, 287, 465, 3331, 1848, 13, 383, 5575, 5733, 5834, 11, 508, 550, 587, 1262, 262, 4401, 12, 33, 6525, 406, 45940, 379, 511, 5103, 5043, 11, 1718, 606, 503, 286, 19133, 290, 2067, 511, 898, 6050, 1028, 262, 1664, 13, 887, 326, 550, 2147, 284, 466, 351, 15890, 13, 198, 6653, 14321, 4444, 287, 13540, 11, 290, 465, 5236, 2761, 547, 12939, 13, 2080, 3139, 287, 262, 3331, 11, 339, 900, 572, 287, 465, 4508, 649, 376, 14877, 5093, 319, 30739, 6957, 13, 2399, 1545, 887, 354, 17552, 274, 3160, 287, 257, 15770, 1477, 20523, 5318, 4803, 287, 5638, 10711, 11, 16033, 11, 290, 339, 447, 247, 67, 3066, 284, 2245, 416, 13, 198, 2215, 15890, 5284, 11, 484, 24070, 6099, 290, 7342, 5701, 1830, 319, 3195, 13, 383, 1306, 1110, 11, 484, 491, 13322, 656, 262, 16479, 284, 12601, 13, 1119, 1816, 572, 319, 511, 898, 329, 257, 981, 13, 15890, 3940, 8169, 1659, 20842, 656, 617, 15715, 44689, 326, 4721, 656, 257, 27737, 17304, 11, 290, 326, 373, 618, 339, 2497, 262, 4599, 439, 2156, 13, 632, 11544, 656, 683, 13, 632, 714, 423, 587, 262, 6833, 39465, 13000, 393, 262, 22491, 6692, 284, 262, 40175, 2615, 11, 475, 4232, 340, 373, 11, 339, 373, 7428, 3371, 326, 2156, 13, 679, 32724, 9342, 1088, 262, 2524, 13, 679, 1043, 262, 1468, 19369, 810, 584, 6832, 550, 587, 11, 290, 339, 7247, 257, 1402, 3240, 550, 1752, 587, 612, 13, 13742, 28384, 287, 15890, 447, 247, 82, 2000, 13, 21276, 656, 262, 27316, 11, 339, 3332, 319, 257, 9067, 13631, 11, 13501, 2966, 416, 465, 16084, 13, 198, 1544, 447, 247, 67, 5298, 262, 2636, 0, 679, 561, 1382, 257, 18761, 1014, 284, 9538, 11, 257, 10066, 284, 1249, 262, 2626, 15625, 284, 36334, 262, 995, 757, 1399, 3737, 21024, 606, 11, 772, 12755, 606, 13, 198, 5308, 13629, 465, 1545, 736, 319, 262, 8025, 257, 1178, 2250, 1568, 11, 339, 1422, 447, 247, 83, 1560, 683, 644, 339, 447, 247, 67, 1043, 11, 1865, 339, 750, 719, 588, 339, 447, 247, 67, 1839, 262, 300, 17631, 393, 523, 13, 887, 354, 1807, 339, 447, 247, 67, 2277, 465, 1182, 319, 257, 3881, 13, 198, 49640, 5839, 262, 3240, 13, 383, 1181, 4409, 287, 5575, 30242, 959, 6292, 465, 14431, 8406, 11, 15975, 257, 13444, 287, 262, 4351, 9473, 13011, 606, 422, 597, 12247, 329, 1997, 326, 1816, 2642, 319, 326, 1956, 13, 679, 4504, 284, 5575, 30242, 959, 1115, 2745, 1568, 351, 257, 3331, 2198, 286, 530, 3470, 7319, 5054, 10398, 503, 284, 262, 1181, 286, 16033, 13, 1770, 13, 1004, 14029, 783, 6898, 12277, 12, 26548, 16051, 286, 1956, 11, 1390, 8759, 263, 32666, 13, 632, 373, 1327, 284, 651, 612, 11, 290, 262, 1295, 550, 645, 8744, 393, 2491, 1660, 11, 475, 339, 9658, 45782, 13, 679, 4762, 465, 16084, 561, 1282, 2081, 13, 198, 49640, 2067, 465, 4320, 351, 257, 14659, 707, 11, 7720, 1497, 262, 625, 27922, 319, 262, 736, 8025, 422, 371, 18320, 32666, 284, 7703, 5866, 5567, 11, 290, 340, 1718, 683, 1115, 1528, 13, 2293, 326, 11, 339, 373, 1498, 284, 25647, 734, 27298, 5556, 257, 1757, 1024, 567, 38278, 351, 257, 736, 38979, 510, 326, 13852, 88, 2610, 284, 262, 32666, 13, 198, 1890, 683, 284, 2652, 287, 262, 4599, 439, 2156, 2950, 13586, 3463, 12, 28655, 26741, 11, 18570, 11, 290, 11087, 7714, 11, 290, 339, 8020, 447, 247, 83, 550, 1576, 640, 284, 651, 326, 1760, 878, 7374, 13, 16238, 11, 262, 5770, 69, 669, 1685, 8025, 373, 991, 407, 6228, 1576, 284, 458, 322, 1865, 13, 679, 9658, 287, 257, 2119, 278, 2156, 287, 5575, 30242, 959, 290, 19036, 625, 4171, 17190, 329, 262, 6076, 5103, 13, 679, 447, 247, 67, 3708, 625, 284, 887, 354, 447, 247, 82, 2156, 5403, 257, 1285, 290, 8181, 503, 11, 475, 287, 3035, 11, 339, 1816, 284, 670, 13, 679, 5292, 284, 2652, 379, 8759, 263, 32666, 1306, 7374, 13, 383, 13647, 2975, 561, 307, 900, 510, 329, 458, 7855, 416, 262, 886, 286, 262, 3931, 13, 198, 12814, 257, 880, 4122, 11, 339, 1043, 1660, 1474, 262, 2156, 13, 679, 6589, 257, 880, 11, 1271, 530, 319, 465, 1351, 13, 3205, 29247, 262, 3403, 287, 290, 1088, 262, 2156, 11, 15890, 3421, 262, 20009, 17078, 16866, 656, 257, 17717, 540, 1363, 25, 734, 32601, 11, 257, 1336, 9592, 11, 5186, 7588, 11, 290, 257, 44250, 13, 3412, 2632, 1531, 4894, 13, 198, 6385, 339, 373, 407, 281, 7068, 4249, 281, 11949, 13, 8759, 263, 32666, 338, 27556, 550, 407, 1282, 503, 2407, 826, 11, 475, 284, 15890, 11, 340, 373, 25023, 13, 1400, 530, 1297, 683, 326, 530, 286, 262, 9753, 3951, 1244, 307, 4622, 37229, 393, 326, 257, 9168, 359, 714, 307, 257, 895, 3130, 261, 572, 2081, 13, 198, 1544, 447, 247, 67, 13055, 530, 2156, 1657, 4171, 290, 1194, 7872, 13, 383, 1334, 547, 2330, 11, 290, 477, 262, 9753, 427, 278, 829, 547, 13791, 13, 383, 9168, 550, 7770, 82, 290, 41160, 11, 290, 257, 1051, 2354, 262, 2166, 10384, 286, 262, 2276, 3650, 3414, 612, 373, 257, 5466, 319, 1465, 1616, 9403, 290, 37113, 13, 383, 4928, 338, 9168, 547, 20239, 351, 34568, 5405, 11, 290, 477, 262, 466, 967, 77, 8158, 547, 20422, 11, 26852, 3723, 588, 3869, 287, 262, 4252, 13, 198, 1532, 257, 22526, 263, 6807, 656, 262, 2626, 7404, 11, 673, 1244, 892, 1243, 547, 3734, 379, 717, 13, 15890, 550, 27527, 3146, 319, 262, 8215, 286, 465, 7777, 11, 31414, 3726, 351, 8208, 12, 30888, 13, 198, 35, 26919, 351, 262, 1628, 3436, 11, 339, 373, 6078, 1630, 625, 262, 3119, 13, 317, 17793, 550, 7334, 1973, 262, 33179, 286, 257, 2156, 11, 38598, 739, 262, 2166, 3420, 13, 679, 8020, 447, 247, 83, 5969, 326, 1865, 11, 290, 1973, 262, 13647, 2975, 11, 257, 5509, 550, 3405, 2265, 704, 2346, 503, 286, 257, 42497, 339, 447, 247, 67, 1364, 287, 262, 19369, 286, 1194, 2615, 13, 383, 13737, 286, 326, 5509, 550, 5445, 257, 1735, 4324, 422, 262, 2641, 503, 13, 16238, 4599, 439, 447, 247, 82, 1295, 11, 262, 691, 584, 2615, 287, 8759, 263, 32666, 351, 257, 1218, 4314, 373, 262, 717, 2156, 339, 447, 247, 67, 3170, 11, 290, 339, 447, 247, 67, 326, 5210, 13, 383, 5637, 7777, 11, 262, 4928, 11, 290, 262, 3650, 550, 645, 11087, 7714, 13, 198, 464, 8222, 373, 38598, 736, 656, 262, 3240, 11, 290, 15890, 373, 4330, 340, 387, 746, 26267, 306, 13, 317, 2330, 35122, 354, 6204, 379, 262, 10384, 286, 465, 4928, 981, 11135, 36193, 656, 9004, 286, 8774, 3530, 13, 383, 13215, 286, 465, 45493, 3240, 547, 30282, 278, 11, 475, 339, 373, 1165, 662, 28756, 284, 1630, 340, 13, 632, 373, 2048, 3492, 13, 2399, 3240, 561, 307, 262, 7696, 329, 262, 24057, 284, 307, 900, 1479, 13, 1867, 561, 257, 1178, 36708, 2300, 30, 198, 18, 25, 1983, 3122, 3909, 11, 1737, 1596], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [10482, 11851, 25, 29274, 7137, 198, 10482, 14041, 25, 29274, 7137, 198, 447, 250, 18943, 9504, 5129, 13488, 329, 606, 379, 262, 5461, 1627, 198, 259, 257, 3234, 326, 40424, 606, 656, 4692, 12, 50132, 5123, 13, 447, 251, 198, 38, 496, 26101, 198, 51, 18047, 7137, 198, 15269, 10673, 3738, 36743, 5857, 11739, 198, 1797, 15766, 25, 41417, 12, 16, 12, 5607, 486, 4310, 12, 2920, 12, 16, 198, 20344, 3890, 25, 44211, 4897, 5834, 198, 27245, 7412, 25, 37356, 198, 2949, 636, 286, 428, 9207, 743, 307, 31759, 393, 18307, 287, 597, 1296, 393, 416, 597, 1724, 11, 13028, 11, 7914, 11, 49857, 11081, 11, 8296, 11, 393, 416, 597, 1321, 6143, 45069, 1080, 960, 16341, 329, 39146, 973, 329, 3199, 2423, 960, 19419, 262, 3194, 7170, 286, 262, 6434, 13, 198, 14772, 6669, 1653, 23499, 198, 53, 3529, 8511, 11, 4744, 198, 464, 38315, 2304, 385, 2254, 198, 75, 1689, 1653, 12984, 20020, 31, 14816, 13, 785, 198, 35, 276, 3474, 284, 6219, 38005, 198, 2964, 75, 5119, 198, 1157, 25, 2327, 3001, 3431, 11, 1737, 1511, 11, 12113, 198, 49, 15799, 32666, 11, 16033, 198, 47, 7958, 262, 2524, 1022, 734, 1402, 18180, 11, 484, 3706, 23915, 290, 38681, 17704, 26, 8759, 263, 32666, 373, 5201, 287, 1596, 5999, 13, 1119, 3170, 1115, 1036, 396, 76, 2171, 1262, 262, 1176, 286, 262, 18180, 284, 20078, 262, 13020, 13, 11597, 3111, 880, 1576, 1566, 257, 6947, 6572, 749, 286, 511, 5682, 290, 477, 262, 41999, 287, 1248, 1983, 13, 1320, 1241, 286, 8166, 1908, 606, 1497, 11, 290, 340, 373, 922, 484, 447, 247, 67, 795, 38769, 503, 13, 317, 1218, 6388, 7560, 1194, 24726, 6947, 5193, 812, 1568, 11, 20518, 2048, 2279, 2073, 1497, 13, 1318, 2492, 447, 247, 83, 881, 286, 262, 3240, 1364, 287, 262, 4295, 16479, 286, 16033, 25, 257, 10905, 3240, 1327, 284, 1064, 13, 198, 464, 5638, 10711, 40425, 10840, 373, 287, 7703, 5866, 1812, 3250, 13, 8759, 263, 32666, 373, 1440, 4608, 1497, 422, 262, 22115, 13, 1649, 262, 1181, 19233, 262, 1956, 1088, 262, 22115, 11, 340, 6492, 262, 3240, 832, 37820, 7386, 13, 16847, 44001, 749, 286, 262, 32666, 287, 9507, 11, 351, 691, 530, 6631, 13, 29602, 287, 5638, 10711, 12086, 8759, 263, 32666, 11, 290, 484, 8853, 276, 262, 1957, 1230, 284, 1309, 25017, 4599, 439, 447, 247, 82, 2156, 1302, 13, 383, 6156, 26765, 3332, 319, 262, 4511, 22910, 1022, 262, 18180, 11, 6193, 278, 832, 262, 27283, 13, 5334, 6961, 373, 6292, 11, 5051, 4035, 262, 2156, 355, 257, 6754, 17757, 13, 1400, 530, 561, 766, 4599, 439, 447, 247, 82, 2156, 706, 262, 30164, 3259, 1364, 262, 1989, 11, 523, 340, 1107, 373, 257, 27158, 28251, 13, 1629, 1551, 484, 1392, 262, 1334, 286, 262, 670, 1760, 5443, 290, 11721, 13, 198, 40630, 40908, 276, 656, 4647, 13, 1400, 530, 2982, 262, 1468, 2156, 1126, 461, 290, 19680, 13, 1400, 530, 1816, 503, 612, 13, 1400, 530, 12086, 262, 1295, 7471, 13, 383, 28902, 422, 262, 1036, 396, 76, 2171, 550, 9292, 284, 262, 4220, 286, 262, 18180, 11, 655, 23273, 11945, 355, 262, 2877, 9353, 286, 262, 8222, 356, 9586, 656, 373, 1364, 286, 8759, 263, 32666, 284, 14083, 290, 23875, 13, 383, 4599, 439, 1363, 28178, 625, 49333, 1399, 28446, 21083, 28077, 257, 10574, 10436, 11, 33971, 257, 2700, 326, 550, 587, 46681, 77, 803, 612, 329, 4647, 13, 198, 38454, 8369, 7521, 319, 262, 7714, 286, 465, 4928, 16968, 683, 510, 13, 15890, 1718, 465, 10147, 572, 11, 37432, 340, 319, 257, 5445, 13990, 1281, 11, 3772, 284, 3443, 670, 287, 257, 309, 12, 15600, 13, 632, 447, 247, 82, 587, 1165, 4692, 284, 466, 326, 329, 1933, 13, 2293, 1115, 812, 286, 670, 11, 465, 3240, 373, 2048, 1760, 960, 4053, 11, 262, 24685, 16175, 671, 286, 340, 6949, 13, 679, 447, 247, 67, 3170, 1936, 7777, 11, 257, 2276, 3650, 11, 290, 257, 4928, 319, 281, 8593, 9586, 2610, 339, 1444, 564, 246, 12417, 4675, 13, 447, 247, 198, 1544, 1234, 262, 7521, 2485, 866, 290, 1761, 9124, 625, 284, 262, 3996, 286, 465, 7779, 284, 1064, 465, 17626, 13, 1119, 547, 29445, 2004, 739, 465, 2891, 3524, 13, 679, 6578, 530, 290, 23831, 1028, 262, 1735, 286, 465, 376, 14877, 11, 33679, 278, 503, 7523, 13917, 13, 1629, 16571, 12, 11545, 11, 465, 1767, 550, 7082, 46258, 306, 284, 465, 22286, 11, 290, 465, 4190, 290, 467, 378, 68, 691, 550, 257, 1178, 48356, 286, 13791, 284, 12127, 883, 812, 13, 198, 43, 3543, 287, 257, 2717, 3770, 287, 6035, 10711, 11, 14493, 11, 422, 16101, 284, 15524, 11, 15890, 1004, 14029, 550, 587, 900, 1479, 826, 706, 465, 294, 2265, 19235, 10955, 13, 679, 447, 247, 67, 4504, 284, 5103, 11, 7463, 8208, 12, 26022, 3625, 572, 257, 18002, 319, 257, 1693, 287, 510, 5219, 968, 1971, 287, 14745, 11, 7163, 465, 10359, 290, 14511, 13, 679, 373, 287, 257, 33658, 329, 625, 257, 1227, 13, 383, 5096, 1664, 373, 29793, 276, 618, 339, 19092, 510, 13, 383, 22536, 12, 354, 2313, 9326, 2952, 1043, 1223, 2642, 351, 262, 18002, 13, 383, 1664, 290, 262, 8517, 1111, 10282, 503, 286, 2184, 11, 5989, 683, 5193, 1510, 5054, 287, 1128, 24355, 257, 614, 706, 262, 5778, 13, 15890, 4444, 510, 351, 1440, 1510, 5054, 287, 465, 3331, 1848, 13, 383, 5575, 5733, 5834, 11, 508, 550, 587, 1262, 262, 4401, 12, 33, 6525, 406, 45940, 379, 511, 5103, 5043, 11, 1718, 606, 503, 286, 19133, 290, 2067, 511, 898, 6050, 1028, 262, 1664, 13, 887, 326, 550, 2147, 284, 466, 351, 15890, 13, 198, 6653, 14321, 4444, 287, 13540, 11, 290, 465, 5236, 2761, 547, 12939, 13, 2080, 3139, 287, 262, 3331, 11, 339, 900, 572, 287, 465, 4508, 649, 376, 14877, 5093, 319, 30739, 6957, 13, 2399, 1545, 887, 354, 17552, 274, 3160, 287, 257, 15770, 1477, 20523, 5318, 4803, 287, 5638, 10711, 11, 16033, 11, 290, 339, 447, 247, 67, 3066, 284, 2245, 416, 13, 198, 2215, 15890, 5284, 11, 484, 24070, 6099, 290, 7342, 5701, 1830, 319, 3195, 13, 383, 1306, 1110, 11, 484, 491, 13322, 656, 262, 16479, 284, 12601, 13, 1119, 1816, 572, 319, 511, 898, 329, 257, 981, 13, 15890, 3940, 8169, 1659, 20842, 656, 617, 15715, 44689, 326, 4721, 656, 257, 27737, 17304, 11, 290, 326, 373, 618, 339, 2497, 262, 4599, 439, 2156, 13, 632, 11544, 656, 683, 13, 632, 714, 423, 587, 262, 6833, 39465, 13000, 393, 262, 22491, 6692, 284, 262, 40175, 2615, 11, 475, 4232, 340, 373, 11, 339, 373, 7428, 3371, 326, 2156, 13, 679, 32724, 9342, 1088, 262, 2524, 13, 679, 1043, 262, 1468, 19369, 810, 584, 6832, 550, 587, 11, 290, 339, 7247, 257, 1402, 3240, 550, 1752, 587, 612, 13, 13742, 28384, 287, 15890, 447, 247, 82, 2000, 13, 21276, 656, 262, 27316, 11, 339, 3332, 319, 257, 9067, 13631, 11, 13501, 2966, 416, 465, 16084, 13, 198, 1544, 447, 247, 67, 5298, 262, 2636, 0, 679, 561, 1382, 257, 18761, 1014, 284, 9538, 11, 257, 10066, 284, 1249, 262, 2626, 15625, 284, 36334, 262, 995, 757, 1399, 3737, 21024, 606, 11, 772, 12755, 606, 13, 198, 5308, 13629, 465, 1545, 736, 319, 262, 8025, 257, 1178, 2250, 1568, 11, 339, 1422, 447, 247, 83, 1560, 683, 644, 339, 447, 247, 67, 1043, 11, 1865, 339, 750, 719, 588, 339, 447, 247, 67, 1839, 262, 300, 17631, 393, 523, 13, 887, 354, 1807, 339, 447, 247, 67, 2277, 465, 1182, 319, 257, 3881, 13, 198, 49640, 5839, 262, 3240, 13, 383, 1181, 4409, 287, 5575, 30242, 959, 6292, 465, 14431, 8406, 11, 15975, 257, 13444, 287, 262, 4351, 9473, 13011, 606, 422, 597, 12247, 329, 1997, 326, 1816, 2642, 319, 326, 1956, 13, 679, 4504, 284, 5575, 30242, 959, 1115, 2745, 1568, 351, 257, 3331, 2198, 286, 530, 3470, 7319, 5054, 10398, 503, 284, 262, 1181, 286, 16033, 13, 1770, 13, 1004, 14029, 783, 6898, 12277, 12, 26548, 16051, 286, 1956, 11, 1390, 8759, 263, 32666, 13, 632, 373, 1327, 284, 651, 612, 11, 290, 262, 1295, 550, 645, 8744, 393, 2491, 1660, 11, 475, 339, 9658, 45782, 13, 679, 4762, 465, 16084, 561, 1282, 2081, 13, 198, 49640, 2067, 465, 4320, 351, 257, 14659, 707, 11, 7720, 1497, 262, 625, 27922, 319, 262, 736, 8025, 422, 371, 18320, 32666, 284, 7703, 5866, 5567, 11, 290, 340, 1718, 683, 1115, 1528, 13, 2293, 326, 11, 339, 373, 1498, 284, 25647, 734, 27298, 5556, 257, 1757, 1024, 567, 38278, 351, 257, 736, 38979, 510, 326, 13852, 88, 2610, 284, 262, 32666, 13, 198, 1890, 683, 284, 2652, 287, 262, 4599, 439, 2156, 2950, 13586, 3463, 12, 28655, 26741, 11, 18570, 11, 290, 11087, 7714, 11, 290, 339, 8020, 447, 247, 83, 550, 1576, 640, 284, 651, 326, 1760, 878, 7374, 13, 16238, 11, 262, 5770, 69, 669, 1685, 8025, 373, 991, 407, 6228, 1576, 284, 458, 322, 1865, 13, 679, 9658, 287, 257, 2119, 278, 2156, 287, 5575, 30242, 959, 290, 19036, 625, 4171, 17190, 329, 262, 6076, 5103, 13, 679, 447, 247, 67, 3708, 625, 284, 887, 354, 447, 247, 82, 2156, 5403, 257, 1285, 290, 8181, 503, 11, 475, 287, 3035, 11, 339, 1816, 284, 670, 13, 679, 5292, 284, 2652, 379, 8759, 263, 32666, 1306, 7374, 13, 383, 13647, 2975, 561, 307, 900, 510, 329, 458, 7855, 416, 262, 886, 286, 262, 3931, 13, 198, 12814, 257, 880, 4122, 11, 339, 1043, 1660, 1474, 262, 2156, 13, 679, 6589, 257, 880, 11, 1271, 530, 319, 465, 1351, 13, 3205, 29247, 262, 3403, 287, 290, 1088, 262, 2156, 11, 15890, 3421, 262, 20009, 17078, 16866, 656, 257, 17717, 540, 1363, 25, 734, 32601, 11, 257, 1336, 9592, 11, 5186, 7588, 11, 290, 257, 44250, 13, 3412, 2632, 1531, 4894, 13, 198, 6385, 339, 373, 407, 281, 7068, 4249, 281, 11949, 13, 8759, 263, 32666, 338, 27556, 550, 407, 1282, 503, 2407, 826, 11, 475, 284, 15890, 11, 340, 373, 25023, 13, 1400, 530, 1297, 683, 326, 530, 286, 262, 9753, 3951, 1244, 307, 4622, 37229, 393, 326, 257, 9168, 359, 714, 307, 257, 895, 3130, 261, 572, 2081, 13, 198, 1544, 447, 247, 67, 13055, 530, 2156, 1657, 4171, 290, 1194, 7872, 13, 383, 1334, 547, 2330, 11, 290, 477, 262, 9753, 427, 278, 829, 547, 13791, 13, 383, 9168, 550, 7770, 82, 290, 41160, 11, 290, 257, 1051, 2354, 262, 2166, 10384, 286, 262, 2276, 3650, 3414, 612, 373, 257, 5466, 319, 1465, 1616, 9403, 290, 37113, 13, 383, 4928, 338, 9168, 547, 20239, 351, 34568, 5405, 11, 290, 477, 262, 466, 967, 77, 8158, 547, 20422, 11, 26852, 3723, 588, 3869, 287, 262, 4252, 13, 198, 1532, 257, 22526, 263, 6807, 656, 262, 2626, 7404, 11, 673, 1244, 892, 1243, 547, 3734, 379, 717, 13, 15890, 550, 27527, 3146, 319, 262, 8215, 286, 465, 7777, 11, 31414, 3726, 351, 8208, 12, 30888, 13, 198, 35, 26919, 351, 262, 1628, 3436, 11, 339, 373, 6078, 1630, 625, 262, 3119, 13, 317, 17793, 550, 7334, 1973, 262, 33179, 286, 257, 2156, 11, 38598, 739, 262, 2166, 3420, 13, 679, 8020, 447, 247, 83, 5969, 326, 1865, 11, 290, 1973, 262, 13647, 2975, 11, 257, 5509, 550, 3405, 2265, 704, 2346, 503, 286, 257, 42497, 339, 447, 247, 67, 1364, 287, 262, 19369, 286, 1194, 2615, 13, 383, 13737, 286, 326, 5509, 550, 5445, 257, 1735, 4324, 422, 262, 2641, 503, 13, 16238, 4599, 439, 447, 247, 82, 1295, 11, 262, 691, 584, 2615, 287, 8759, 263, 32666, 351, 257, 1218, 4314, 373, 262, 717, 2156, 339, 447, 247, 67, 3170, 11, 290, 339, 447, 247, 67, 326, 5210, 13, 383, 5637, 7777, 11, 262, 4928, 11, 290, 262, 3650, 550, 645, 11087, 7714, 13, 198, 464, 8222, 373, 38598, 736, 656, 262, 3240, 11, 290, 15890, 373, 4330, 340, 387, 746, 26267, 306, 13, 317, 2330, 35122, 354, 6204, 379, 262, 10384, 286, 465, 4928, 981, 11135, 36193, 656, 9004, 286, 8774, 3530, 13, 383, 13215, 286, 465, 45493, 3240, 547, 30282, 278, 11, 475, 339, 373, 1165, 662, 28756, 284, 1630, 340, 13, 632, 373, 2048, 3492, 13, 2399, 3240, 561, 307, 262, 7696, 329, 262, 24057, 284, 307, 900, 1479, 13, 1867, 561, 257, 1178, 36708, 2300, 30, 198, 18, 25, 1983, 3122, 3909, 11, 1737, 1596]}\n",
      "Training dataset size: 72\n",
      "Validation dataset size: 18\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForLanguageModeling # Keep import for later use if needed\n",
    "\n",
    "def combine_text(row):\n",
    "    return f\"Book Title: {row['bookTitle']}\\nBook Content: {row['bookContent']}\\nReview: {row['reviews']}\"\n",
    "\n",
    "merged_df['combined_text'] = merged_df.apply(combine_text, axis=1)\n",
    "\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(merged_df)\n",
    "\n",
    "# Set a padding token for the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the combined text with truncation but without padding here\n",
    "def tokenize_function(examples):\n",
    "    # Use the tokenizer loaded earlier\n",
    "    # Set truncation to True to ensure sequences are not longer than model_max_length\n",
    "    # Remove padding=\"max_length\" from this step\n",
    "    return tokenizer(examples[\"combined_text\"], truncation=True, max_length=model_max_length)\n",
    "\n",
    "# Remove '__index_level_0__' from the remove_columns list (if it exists after using merged_df)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['bookTitle', 'reviews', 'bookContent', 'combined_text'])\n",
    "\n",
    "# Data blocking: Concatenate and split into fixed-size chunks\n",
    "# This ensures all input sequences have the same length (model_max_length)\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts from the batch\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # Drop the last incomplete chunk if it exists\n",
    "    total_length = (total_length // model_max_length) * model_max_length\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + model_max_length] for i in range(0, total_length, model_max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy() # For causal language modeling, labels are the same as input_ids\n",
    "    return result\n",
    "\n",
    "# Apply the data blocking function to the tokenized dataset\n",
    "lm_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000, # Process in larger batches for efficiency\n",
    "    num_proc=4, # Use multiple processes for faster mapping\n",
    ")\n",
    "\n",
    "\n",
    "# Print some information about the processed dataset\n",
    "print(lm_dataset)\n",
    "print(lm_dataset[0])\n",
    "\n",
    "# Split the blocked dataset into training and validation sets\n",
    "train_test_split = lm_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14642/2649808024.py:12: DeprecationWarning: Use torch_xla.device instead\n",
      "  if _has_tpu: device = xm.xla_device()\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "# You might need to install and import torch_xla for actual TPU detection and usage\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    _has_tpu = True\n",
    "except ImportError:\n",
    "    _has_tpu = False\n",
    "\n",
    "device = xm.xla_device() if _has_tpu else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set batch sizes based on device\n",
    "# Assuming 'cuda' for GPU, 'cpu' for CPU, and a hypothetical 'tpu'\n",
    "# You would need to adjust this logic based on how you detect your TPU\n",
    "# current_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# if _has_tpu: current_device = \"tpu\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    train_batch_size = 8  # Reduced batch size for GPU\n",
    "    eval_batch_size = 8 # Reduced batch size for GPU\n",
    "elif _has_tpu: # Check device type for XLA (TPU)\n",
    "    # TPU batch size is typically per core, and you have multiple cores (e.g., 8 for v2-8)\n",
    "    # You would set a per-core batch size and the Trainer handles distribution\n",
    "    # Let's use a larger per-core batch size for TPU example\n",
    "    train_batch_size = 7 # Example per-core batch size for TPU\n",
    "    eval_batch_size = 7 # Example per-core batch size for TPU\n",
    "else: # Default to CPU\n",
    "    train_batch_size = 1  # Smaller batch size for CPU\n",
    "    eval_batch_size = 1 # Smaller batch size for CPU\n",
    "\n",
    "'''\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "output_dir=\"./output/gpt-neo-fine-tuned\",  # Directory to save checkpoints and logs\n",
    "overwrite_output_dir=True,\n",
    "num_train_epochs=3,  # Number of training epochs\n",
    "per_device_train_batch_size=train_batch_size,  # Batch size for training based on device\n",
    "per_device_eval_batch_size=eval_batch_size,  # Batch size for evaluation based on device\n",
    "learning_rate=5e-5,  # Learning rate\n",
    "weight_decay=0.01,\n",
    "eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "save_strategy=\"epoch\", # Save checkpoint at the end of each epoch\n",
    "load_best_model_at_end=True, # Load the best model based on evaluation metric at the end\n",
    "metric_for_best_model=\"eval_loss\", # Metric to monitor for best model\n",
    "logging_dir=\"./logs\", # Directory for storing logs\n",
    "logging_steps=10, # Log training progress every 10 steps\n",
    "report_to=\"none\" # Disable reporting to external services like Weights & Biases\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "output_dir=\"./output/gpt-neo-fine-tuned\",  # Directory to save checkpoints and logs\n",
    "overwrite_output_dir=True,\n",
    "num_train_epochs=3,  # Number of training epochs\n",
    "per_device_train_batch_size=train_batch_size,  # Batch size for training based on device\n",
    "per_device_eval_batch_size=eval_batch_size,  # Batch size for evaluation based on device\n",
    "learning_rate=5e-5,  # Learning rate\n",
    "weight_decay=0.01,\n",
    "eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "save_strategy=\"epoch\", # Save checkpoint at the end of each epoch\n",
    "load_best_model_at_end=True, # Load the best model based on evaluation metric at the end\n",
    "metric_for_best_model=\"eval_loss\", # Metric to monitor for best model\n",
    "logging_dir=\"./logs\", # Directory for storing logs\n",
    "logging_steps=10, # Log training progress every 10 steps\n",
    "report_to=\"none\", # Disable reporting to external services like Weights & Biases\n",
    "fp16=False, # Ensure fp16 is not used by default\n",
    "bf16=False # Explicitly disable bf16\n",
    ")\n",
    "print(training_args)\n",
    "'''\n",
    "training_args = TrainingArguments(\n",
    "output_dir=\"./output/gpt-neo-fine-tuned\",  # Directory to save checkpoints and logs\n",
    "overwrite_output_dir=True,\n",
    "num_train_epochs=3,  # Number of training epochs\n",
    "per_device_train_batch_size=train_batch_size,  # Batch size for training based on device\n",
    "per_device_eval_batch_size=eval_batch_size,  # Batch size for evaluation based on device\n",
    "learning_rate=5e-5,  # Learning rate\n",
    "weight_decay=0.01,\n",
    "eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "save_strategy=\"epoch\", # Save checkpoint at the end of each epoch\n",
    "load_best_model_at_end=True, # Load the best model based on evaluation metric at the end\n",
    "metric_for_best_model=\"eval_loss\", # Metric to monitor for best model\n",
    "logging_dir=\"./logs\", # Directory for storing logs\n",
    "logging_steps=10, # Log training progress every 10 steps\n",
    "report_to=\"none\", # Disable reporting to external services like Weights & Biases\n",
    "fp16=False, # Ensure fp16 is not used by default\n",
    "bf16=False # Explicitly disable bf16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cede2ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/929585827.py:9: DeprecationWarning: Use torch_xla.device instead\n",
      "  device = xm.xla_device() if _has_tpu else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    _has_tpu = True\n",
    "except ImportError:\n",
    "    _has_tpu = False\n",
    "\n",
    "# Set the device to TPU if available\n",
    "device = xm.xla_device() if _has_tpu else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the correct device\n",
    "model.to(device)\n",
    "\n",
    "# Ensure the model is in `float32` (default for TPU)\n",
    "model = model.to(torch.float32)  # Set to float32 explicitly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "298b1561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "`fused=True` requires all the params to be floating point Tensors of supported devices: ['mps', 'cuda', 'xpu', 'hpu', 'cpu', 'privateuseone'] but torch.float32 and xla",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m trainer = accelerator.prepare(trainer)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/transformers/trainer.py:2648\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2645\u001b[39m     context = implicit_replication\n\u001b[32m   2647\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2648\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2652\u001b[39m \u001b[38;5;66;03m# get leaning rate before update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/accelerate/optimizer.py:179\u001b[39m, in \u001b[36mAcceleratedOptimizer.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    177\u001b[39m         \u001b[38;5;28mself\u001b[39m._accelerate_step_called = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator_state.distributed_type == DistributedType.XLA:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.gradient_state.is_xla_gradients_synced = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:133\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m opt = opt_ref()\n\u001b[32m    132\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/adam.py:237\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     state_steps: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     has_complex = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     adam(\n\u001b[32m    248\u001b[39m         params_with_grad,\n\u001b[32m    249\u001b[39m         grads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    268\u001b[39m         decoupled_weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mdecoupled_weight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    269\u001b[39m     )\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/adam.py:163\u001b[39m, in \u001b[36mAdam._init_group\u001b[39m\u001b[34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) == \u001b[32m0\u001b[39m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[33m\"\u001b[39m\u001b[33mfused\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m         \u001b[43m_device_dtype_check_for_fused\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# note(crcrpar): [special device hosting for step]\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Deliberately host `step` on CPU if both capturable and fused are off.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# This is because kernel launches are costly on CUDA and XLA.\u001b[39;00m\n\u001b[32m    167\u001b[39m     state[\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m    168\u001b[39m         torch.zeros(\n\u001b[32m    169\u001b[39m             (),\n\u001b[32m   (...)\u001b[39m\u001b[32m    174\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m torch.tensor(\u001b[32m0.0\u001b[39m, dtype=_get_scalar_dtype())\n\u001b[32m    175\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:197\u001b[39m, in \u001b[36m_device_dtype_check_for_fused\u001b[39m\u001b[34m(p, cuda_unsupported)\u001b[39m\n\u001b[32m    195\u001b[39m     fused_supported_devices.remove(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (p.device.type \u001b[38;5;129;01min\u001b[39;00m fused_supported_devices \u001b[38;5;129;01mand\u001b[39;00m torch.is_floating_point(p)):\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`fused=True` requires all the params to be floating point Tensors of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msupported devices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfused_supported_devices\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp.device.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: `fused=True` requires all the params to be floating point Tensors of supported devices: ['mps', 'cuda', 'xpu', 'hpu', 'cpu', 'privateuseone'] but torch.float32 and xla"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "# Initialize Accelerator with no mixed precision\n",
    "accelerator = Accelerator(mixed_precision=\"no\")  # Explicitly disable mixed precision\n",
    "\n",
    "# Set device to TPU\n",
    "device = 'cpu'\n",
    "\n",
    "# Move the model to TPU and ensure float32 precision\n",
    "model = model.to(device).to(torch.float32)\n",
    "'''\n",
    "# Define TrainingArguments with no mixed precision enabled\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output/gpt-neo-fine-tuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    fp16=False,  # Disable FP16 precision\n",
    "    bf16=False,  # Disable BF16 precision\n",
    "    report_to=\"none\",  # Disable external reporting\n",
    ")\n",
    "'''\n",
    "# Create Trainer with no mixed precision and manually disable any fused operations\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Your training dataset\n",
    "    eval_dataset=eval_dataset,  # Your evaluation dataset\n",
    ")\n",
    "\n",
    "# Prepare the trainer with accelerator (no mixed precision)\n",
    "trainer = accelerator.prepare(trainer)\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c143a4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28316/1485053130.py:10: DeprecationWarning: Use torch_xla.device instead\n",
      "  device = xm.xla_device() if _has_tpu else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: xla:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n",
      "WARNING:root:You are trying to use a feature that requires jax/pallas.You can install Jax/Pallas via pip install torch_xla[pallas]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "`fused=True` requires all the params to be floating point Tensors of supported devices: ['mps', 'cuda', 'xpu', 'hpu', 'cpu', 'privateuseone'] but torch.float32 and xla",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.args.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Save the model after training\u001b[39;00m\n\u001b[32m     56\u001b[39m trainer.save_model(\u001b[33m\"\u001b[39m\u001b[33m./fine_tuned_model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/transformers/trainer.py:2648\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2645\u001b[39m     context = implicit_replication\n\u001b[32m   2647\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2648\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2652\u001b[39m \u001b[38;5;66;03m# get leaning rate before update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/accelerate/optimizer.py:179\u001b[39m, in \u001b[36mAcceleratedOptimizer.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    177\u001b[39m         \u001b[38;5;28mself\u001b[39m._accelerate_step_called = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator_state.distributed_type == DistributedType.XLA:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.gradient_state.is_xla_gradients_synced = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:133\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m opt = opt_ref()\n\u001b[32m    132\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/adam.py:237\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     state_steps: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     has_complex = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     adam(\n\u001b[32m    248\u001b[39m         params_with_grad,\n\u001b[32m    249\u001b[39m         grads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    268\u001b[39m         decoupled_weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mdecoupled_weight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    269\u001b[39m     )\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/adam.py:163\u001b[39m, in \u001b[36mAdam._init_group\u001b[39m\u001b[34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) == \u001b[32m0\u001b[39m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[33m\"\u001b[39m\u001b[33mfused\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m         \u001b[43m_device_dtype_check_for_fused\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# note(crcrpar): [special device hosting for step]\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Deliberately host `step` on CPU if both capturable and fused are off.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# This is because kernel launches are costly on CUDA and XLA.\u001b[39;00m\n\u001b[32m    167\u001b[39m     state[\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m    168\u001b[39m         torch.zeros(\n\u001b[32m    169\u001b[39m             (),\n\u001b[32m   (...)\u001b[39m\u001b[32m    174\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m torch.tensor(\u001b[32m0.0\u001b[39m, dtype=_get_scalar_dtype())\n\u001b[32m    175\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Online-Book-Club-Review/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:197\u001b[39m, in \u001b[36m_device_dtype_check_for_fused\u001b[39m\u001b[34m(p, cuda_unsupported)\u001b[39m\n\u001b[32m    195\u001b[39m     fused_supported_devices.remove(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (p.device.type \u001b[38;5;129;01min\u001b[39;00m fused_supported_devices \u001b[38;5;129;01mand\u001b[39;00m torch.is_floating_point(p)):\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`fused=True` requires all the params to be floating point Tensors of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msupported devices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfused_supported_devices\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp.device.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: `fused=True` requires all the params to be floating point Tensors of supported devices: ['mps', 'cuda', 'xpu', 'hpu', 'cpu', 'privateuseone'] but torch.float32 and xla"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    _has_tpu = True\n",
    "except ImportError:\n",
    "    _has_tpu = False\n",
    "\n",
    "# Set the device to TPU if available\n",
    "device = xm.xla_device() if _has_tpu else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Adjust training batch size based on device\n",
    "if device == \"cuda\":\n",
    "    train_batch_size = 8\n",
    "    eval_batch_size = 8\n",
    "elif device.type == \"xla\":  # TPU-specific handling\n",
    "    train_batch_size = 7  # Adjust per-core batch size\n",
    "    eval_batch_size = 7\n",
    "else:\n",
    "    train_batch_size = 1\n",
    "    eval_batch_size = 1\n",
    "\n",
    "# Ensure no mixed precision unless needed, avoid `fused=True`\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output/gpt-neo-fine-tuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    fp16=False,  # Disable FP16 mixed precision\n",
    "    bf16=False,  # Disable BF16 mixed precision\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with the TPU device\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),  # Ensure model is moved to the correct device (TPU or CUDA)\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(f\"Training on device: {trainer.args.device}\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model after training\n",
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Explicitly move the model to the determined device before initializing the Trainer\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model, # Use the model loaded earlier\n",
    "    args=training_args, # Use the training arguments defined earlier\n",
    "    train_dataset=train_dataset, # Use the prepared training dataset\n",
    "    eval_dataset=eval_dataset, # Use the prepared evaluation dataset\n",
    "    # Data collator is usually needed for dynamic padding, but since we did data blocking\n",
    "    # to fixed length, it might not be strictly necessary, but it's good practice.\n",
    "    # We can add DataCollatorForLanguageModeling if needed, but let's try without first.\n",
    "    # data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(f\"Training on device: {trainer.args.device}\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# You can save the fine-tuned model if needed\n",
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0f47c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected device: cpu. Proceeding without Unsloth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "F0000 00:00:1756816466.327917   14642 pjrt_computation_client.cpp:525] Non-OK-status: status\n",
      "Status: INTERNAL: Error preparing computation: Out of memory allocating 36233592832 bytes.\n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\ttorch_xla::runtime::PjRtComputationClient::TransferFromDevice(absl::lts_20230802::Span<std::shared_ptr<torch_xla::runtime::ComputationClient::Data> const>)\n",
      "\ttorch_xla::ReleaseGilAndTransferData(absl::lts_20230802::Span<std::shared_ptr<torch::lazy::BackendData> const>)\n",
      "\ttorch_xla::XlaDataToTensors(absl::lts_20230802::Span<std::shared_ptr<torch::lazy::BackendData> const>, absl::lts_20230802::Span<c10::ScalarType const>)\n",
      "\ttorch_xla::XLATensor::ToTensor(bool)\n",
      "\ttorch_xla::XLANativeFunctions::_to_copy(at::Tensor const&, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, bool, std::optional<c10::MemoryFormat>)\n",
      "\t\n",
      "\tat::_ops::_to_copy::redispatch(c10::DispatchKeySet, at::Tensor const&, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, bool, std::optional<c10::MemoryFormat>)\n",
      "\t\n",
      "\tat::_ops::_to_copy::call(at::Tensor const&, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, bool, std::optional<c10::MemoryFormat>)\n",
      "\t\n",
      "\t\n",
      "\tat::_ops::_to_copy::redispatch(c10::DispatchKeySet, at::Tensor const&, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, bool, std::optional<c10::MemoryFormat>)\n",
      "\t\n",
      "\t\n",
      "\tat::_ops::_to_copy::call(at::Tensor const&, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, bool, std::optional<c10::MemoryFormat>)\n",
      "\tat::native::to(at::Tensor const&, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, bool, bool, std::optional<c10::MemoryFormat>)\n",
      "\t\n",
      "\tat::_ops::to_dtype_layout::call(at::Tensor const&, std::optional<c10::ScalarType>, std::optional<c10::Layout>, std::optional<c10::Device>, std::optional<bool>, bool, bool, std::optional<c10::MemoryFormat>)\n",
      "\tat::Tensor::to(c10::TensorOptions, bool, bool, std::optional<c10::MemoryFormat>) const\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\tPyObject_Vectorcall\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\tPyEval_EvalCode\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\tPyObject_Call\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\tPyEval_EvalCode\n",
      "\t\n",
      "\t\n",
      "\tPyObject_Vectorcall\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\tPy_BytesMain\n",
      "\t\n",
      "\t__libc_start_main\n",
      "\t_start\n",
      "*** End stack trace ***\n",
      "Failed to await future from buffer to literal inTransferFromDevice\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling # Import DataCollatorForLanguageModeling\n",
    "from accelerate import Accelerator\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"Detected device: {device}. Attempting to use Unsloth.\")\n",
    "    # Import Unsloth components\n",
    "    try:\n",
    "        import unsloth\n",
    "        from unsloth import FastLanguageModel\n",
    "        if 'FastTrainer' in dir(unsloth): \n",
    "            from unsloth import FastTrainer\n",
    "            Trainer_class = FastTrainer\n",
    "        elif 'UnslothTrainer' in dir(unsloth):\n",
    "            from unsloth import UnslothTrainer\n",
    "            Trainer_class = UnslothTrainer\n",
    "        else: Trainer_class = Trainer\n",
    "        # Wrap the model with Unsloth\n",
    "        # You might need to adjust max_seq_length based on your tokenized data if not using max_length=model_max_length\n",
    "        try: \n",
    "            model = unsloth.FastLanguageModel.from_pretrained(\n",
    "model_name = model_name, # Use the original model name\n",
    "max_seq_length = model_max_length,\n",
    "dtype = None, # None for auto detection\n",
    "load_in_4bit = True, # Load in 4bit for memory efficiency\n",
    ")\n",
    "            print(\"Model wrapped with Unsloth.\")\n",
    "        except ModuleNotFoundError: pass\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"Unsloth not installed. Proceeding without Unsloth.\")\n",
    "        # If Unsloth is not installed, use the standard Trainer\n",
    "        Trainer_class = Trainer\n",
    "        print(\"Using standard Trainer.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Unsloth setup: {e}. Proceeding with standard Trainer.\")\n",
    "        # If there's an error with Unsloth setup, use the standard Trainer\n",
    "        Trainer_class = Trainer\n",
    "        print(\"Using standard Trainer.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Detected device: {device}. Proceeding without Unsloth.\")\n",
    "    model.to('cpu')\n",
    "\n",
    "    # Use the standard Trainer on CPU\n",
    "    Trainer_class = Trainer\n",
    "    print(\"Using standard Trainer.\")\n",
    "\n",
    "# Define the data collator for causal language modeling\n",
    "# Ensure the data collator is also on the correct device if necessary,\n",
    "# but DataCollatorForLanguageModeling typically handles this during batch preparation\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "# Explicitly pass device to TrainingArguments if available, although Trainer usually handles this\n",
    "# For debugging, setting device=\"cpu\" here might be necessary depending on the Trainer version and how it uses the device\n",
    "trainer = Trainer_class( # Use the determined Trainer class\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator, # Include the data collator\n",
    "    # tokenizer=tokenizer, # Optional, can be left commented out\n",
    "    # compute_metrics=compute_metrics, # Optional, can be left commented out\n",
    ")\n",
    "if device != 'cuda': trainer = accelerator.prepare(trainer)\n",
    "# Start fine-tuning\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# You can save the fine-tuned model if needed\n",
    "trainer.save_model(\"./gpt-neo-fine-tuned\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7354220f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
